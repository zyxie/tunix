# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
#
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from absl.testing import absltest
import jax
from jax import sharding
import jax.numpy as jnp
import numpy as np
from tunix.generate import utils
from tunix.rl import reshard


PartitionSpec = sharding.PartitionSpec
NamedSharding = sharding.NamedSharding
Mesh = sharding.Mesh


class MockState:

  def __init__(self, params):
    self.params = params

  def flat_state(self):
    return [(tuple(k.split(".")), v) for k, v in self.params.items()]

  def from_flat_path(self, flat_path):
    new_params = {}
    for keys, param in flat_path:
      new_params[".".join(keys)] = param.value
    return MockState(new_params)


class MockParam:

  def __init__(self, value):
    self.value = value


class Logprob:

  def __init__(self, logprob, rank=None, decoded_token=None):
    self.logprob = logprob
    self.rank = rank
    self.decoded_token = decoded_token


class UtilsTest(absltest.TestCase):

  def test_compute_attention_mask(self):
    # Check that the input mask is correctly applied when total sampling steps
    # is lower than the max cache length.
    input_mask = jnp.array([[1, 1, 0, 0, 0], [1, 1, 0, 1, 0]], dtype=jnp.bool_)
    seq_len = 8
    time_step = 4
    attn_mask = utils.compute_attention_masks(time_step, seq_len, input_mask)
    expected_attn_mask = jnp.array(
        [[0, 0, 1, 1, 1, 0, 0, 0], [0, 0, 1, 0, 1, 0, 0, 0]], dtype=jnp.bool_
    )

    self.assertTrue((attn_mask.squeeze(1) == expected_attn_mask).all())

    # Check that the input mask is correctly applied when total sampling steps
    # is *longer* than the max cache length.
    seq_len = 4
    time_step = 4
    attn_mask = utils.compute_attention_masks(time_step, seq_len, input_mask)
    expected_attn_mask = jnp.array(
        [[0, 1, 1, 1], [0, 1, 0, 1]], dtype=jnp.bool_
    )

    self.assertTrue((attn_mask.squeeze(1) == expected_attn_mask).all())

  def test_make_causal_attn_mask(self):
    input_mask = jnp.array([[0, 1, 1, 0], [1, 1, 1, 0]])
    attn_mask = utils.make_causal_attn_mask(input_mask, 5)
    expected = jnp.array([
        [
            [0, 0, 0, 0, 0],
            [0, 1, 0, 0, 0],
            [0, 1, 1, 0, 0],
            [0, 1, 1, 0, 0],
        ],
        [
            [1, 0, 0, 0, 0],
            [1, 1, 0, 0, 0],
            [1, 1, 1, 0, 0],
            [1, 1, 1, 0, 0],
        ],
    ])
    np.testing.assert_array_equal(attn_mask, expected)

  def test_next_power_of_2(self):
    self.assertEqual(utils.next_power_of_2(0), 1)
    self.assertEqual(utils.next_power_of_2(2), 2)
    self.assertEqual(utils.next_power_of_2(3), 4)
    self.assertEqual(utils.next_power_of_2(4), 4)
    self.assertEqual(utils.next_power_of_2(5), 8)

  def test_find_first_non_pad_idx(self):
    data = [
        ([1, 2, 3, 4, 5, 6], 0),
        ([0, 0, 1, 2, 3, 4], 2),
        ([0, 1, 2, 3, 0, 0], 1),
    ]
    for ids, expected in data:
      self.assertEqual(
          utils.find_first_non_pad_idx(jnp.array(ids), 0), expected
      )

  def test_find_first_eos_idx(self):
    data = [
        ([1, 2, 3, 4, 5, -1], 5),
        ([1, 2, -1, 4, -1, 0], 2),
        ([1, 2, 3, 4, 5, 6], 6),
    ]
    for ids, expected in data:
      self.assertEqual(utils.find_first_eos_idx(jnp.array(ids), -1), expected)

  def test_find_last_non_pad_idx(self):
    data = [
        ([1, 2, 3, 4, 5, 6], 5),
        ([1, 2, 3, 0, 0, 0], 2),
        ([0, 1, 2, 3, 0, 0], 3),
    ]
    for ids, expected in data:
      self.assertEqual(utils.find_last_non_pad_idx(jnp.array(ids), 0), expected)

  def test_logprobs_basic_extraction(self):
    token_ids = [271, 567, 15166]
    logprobs = [
        {271: Logprob(-1.71), 198: Logprob(-0.52)},
        {567: Logprob(-0.37)},
        {15166: Logprob(0.0)},
    ]
    expected = [-1.71, -0.37, 0.0]
    self.assertEqual(
        utils.get_logprobs_from_vllm_output(token_ids, logprobs),
        expected,
    )

  def test_logprobs_extraction_with_missing_token(self):
    token_ids = [100, 200]
    logprobs = [{101: Logprob(-0.5)}, {200: Logprob(-1.2)}]
    with self.assertRaises(ValueError):
      utils.get_logprobs_from_vllm_output(token_ids, logprobs)

  def test_transfer_state_with_mappings_tranpose_and_sharding_device(self):
    device_count = len(jax.devices())
    assert device_count % 2 == 0, "This example assumes even number of devices"

    devices_array = np.array(jax.devices()).reshape((device_count // 2, 2))
    mesh = Mesh(devices_array, axis_names=("data", "model"))

    src_sharding = NamedSharding(mesh, PartitionSpec(None, "model"))
    tgt_sharding = NamedSharding(mesh, PartitionSpec("data", "model"))
    src_state = MockState({
        "encoder.layer_0.weight": MockParam(
            jax.device_put(
                jnp.arange(16).reshape(2, 8).astype(jnp.float32),
                device=src_sharding,
            ),
        ),
        "encoder.layer_1.weight": MockParam(
            jax.device_put(
                jnp.arange(16, 32).reshape(2, 8).astype(jnp.float32),
                device=src_sharding,
            ),
        ),
    })
    tgt_state = MockState({
        "decoder.layer_0.weight": MockParam(
            jax.device_put(
                jnp.zeros((8, 2), dtype=jnp.float32), device=tgt_sharding
            ),
        ),
        "encoder.layer_0.weight": MockParam(
            jax.device_put(
                jnp.zeros((8, 2), dtype=jnp.float32), device=tgt_sharding
            ),
        ),
    })
    mappings = {
        "encoder.layer_0.weight": ("decoder.layer_0.weight", None),
        "encoder.layer_1.weight": ("encoder.layer_0.weight", None),
    }
    transpose_keys = {
        "weight": (1, 0),
    }
    hook_fns = {
        "encoder.layer_0.weight": lambda x: x * 2,
    }

    new_tgt_state = utils.transfer_state_with_mappings(
        src_state,
        tgt_state,
        key_mappings=mappings,
        key_mapping_hook_fns=hook_fns,
        transpose_keys=transpose_keys,
        reshard_fn=reshard.reshard_pytree,
    )

    expected_layer_0_weight = jnp.arange(16).reshape(2, 8).T * 2
    self.assertTrue(
        jnp.array_equal(
            new_tgt_state.params["decoder.layer_0.weight"],
            expected_layer_0_weight,
        )
    )
    expected_layer_1_weight = jnp.arange(16, 32).reshape(2, 8).T
    self.assertTrue(
        jnp.array_equal(
            new_tgt_state.params["encoder.layer_0.weight"],
            expected_layer_1_weight,
        )
    )
    self.assertEqual(
        new_tgt_state.params["decoder.layer_0.weight"].sharding, tgt_sharding
    )
    self.assertEqual(
        new_tgt_state.params["encoder.layer_0.weight"].sharding, tgt_sharding
    )

  def test_transfer_state_with_padding(self):
    # Create source module with smaller head dim
    src = MockState({"w": MockParam(jnp.ones((2, 4, 64)))})
    dst = MockState({"w": MockParam(jnp.zeros((2, 4, 128)))})

    mappings = {
        "w": ("w", None),
    }

    new_tgt_state = utils.transfer_state_with_mappings(src, dst, mappings)

    # Validate shape
    self.assertEqual(new_tgt_state.params["w"].shape, (2, 4, 128))
    # Validate original values copied correctly
    self.assertTrue(jnp.allclose(new_tgt_state.params["w"][:, :, :64], 1.0))
    # Validate padded values are zero
    self.assertTrue(jnp.allclose(new_tgt_state.params["w"][:, :, 64:], 0.0))

  def test_transfer_state_with_scanned_layers(self):
    """Comprehensive test for scanned layers covering multiple scenarios."""
    num_layers = 3
    embed_dim = 4
    vocab_size = 8
    batch_size = 2

    # Create source state with multiple types of parameters:
    # 1. Scanned weights (layer dim on axis 0)
    # 2. Scanned biases (layer dim on axis 1)
    # 3. Regular embedding - no scanning, direct transfer

    # Scanned weights: shape (num_layers, embed_dim, vocab_size)
    scanned_weights = jnp.stack(
        [
            jnp.full((embed_dim, vocab_size), i + 1, dtype=jnp.float32)
            for i in range(num_layers)
        ],
        axis=0,
    )

    # Scanned biases with layer dim on axis 1:
    # shape (batch_size, num_layers, vocab_size)
    scanned_biases = jnp.stack(
        [
            jnp.full((batch_size, vocab_size), (i + 1) * 10, dtype=jnp.float32)
            for i in range(num_layers)
        ],
        axis=1,
    )

    # Regular parameter (no scanning)
    embedding_weights = jnp.full(
        (vocab_size, embed_dim), 99.0, dtype=jnp.float32
    )

    src_state = MockState({
        "transformer.layers.weight": MockParam(
            scanned_weights
        ),  # Scanned on axis 0
        "transformer.layers.bias": MockParam(
            scanned_biases
        ),  # Scanned on axis 1
        "embedding.weight": MockParam(embedding_weights),  # Regular parameter
    })

    # Create target state with individual layer parameters
    target_params = {
        "embedding.weight": MockParam(
            jnp.zeros((embed_dim, vocab_size), dtype=jnp.float32)
        )
    }

    # Individual layer parameters for scanned weights and biases
    for i in range(num_layers - 1, -1, -1):
      target_params[f"decoder.layer.{i}.weight"] = MockParam(
          jnp.zeros(
              (vocab_size, embed_dim), dtype=jnp.float32
          )  # Transposed shape
      )
      target_params[f"decoder.layer.{i}.bias"] = MockParam(
          jnp.zeros((batch_size, vocab_size), dtype=jnp.float32)
      )

    tgt_state = MockState(target_params)

    # Define mappings for all parameter types
    mappings = {
        # Scanned weight with layer on axis 0, target needs transpose
        "transformer.layers.weight": (
            "decoder.layer.*.weight",
            ("layer", None, None),
        ),
        # Scanned bias with layer on axis 1
        "transformer.layers.bias": (
            "decoder.layer.*.bias",
            (None, "layer", None),
        ),
        # Regular parameter that needs transpose
        "embedding.weight": ("embedding.weight", None),
    }

    # Define transpose operations
    transpose_keys = {"weight": (1, 0)}  # Transpose weight matrices

    # Perform the transfer
    new_tgt_state = utils.transfer_state_with_mappings(
        src_state,
        tgt_state,
        key_mappings=mappings,
        transpose_keys=transpose_keys,
    )

    # Verify scanned weights (axis 0) with transpose
    for layer_idx in range(num_layers):
      layer_key = f"decoder.layer.{layer_idx}.weight"
      transferred = new_tgt_state.params[layer_key]

      # Expected: extract layer from axis 0, then transpose
      extracted_layer = scanned_weights[
          layer_idx
      ]  # Shape: (embed_dim, vocab_size)
      expected = jnp.transpose(
          extracted_layer, (1, 0)
      )  # Shape: (vocab_size, embed_dim)

      self.assertEqual(transferred.shape, (vocab_size, embed_dim))
      self.assertTrue(
          jnp.allclose(
              transferred,
              jnp.full(
                  (vocab_size, embed_dim), layer_idx + 1, dtype=jnp.float32
              ),
          ),
          f"Scanned weight layer {layer_idx} mismatch",
      )

    # Verify scanned biases (axis 1) - no transpose
    for layer_idx in range(num_layers):
      layer_key = f"decoder.layer.{layer_idx}.bias"
      transferred = new_tgt_state.params[layer_key]

      # Expected: extract layer from axis 1
      expected = jnp.full(
          (batch_size, vocab_size), (layer_idx + 1) * 10, dtype=jnp.float32
      )

      self.assertEqual(transferred.shape, (batch_size, vocab_size))
      self.assertTrue(
          jnp.allclose(transferred, expected),
          f"Scanned bias layer {layer_idx} mismatch",
      )

    # Verify regular parameter with transpose
    transferred_embedding = new_tgt_state.params["embedding.weight"]

    self.assertEqual(transferred_embedding.shape, (embed_dim, vocab_size))
    self.assertTrue(
        jnp.allclose(
            transferred_embedding,
            jnp.full((embed_dim, vocab_size), 99.0, dtype=jnp.float32),
        ),
        "Regular parameter with transpose mismatch",
    )
