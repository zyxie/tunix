{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "abdhOBYHqYz6",
      "metadata": {
        "id": "abdhOBYHqYz6"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/tunix/blob/main/examples/grpo_demo_test.ipynb\" ><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This tutorial demonstrates training the Gemma 2 2B-IT model on the GSM8K math\n",
        "reasoning benchmark using Group Relative Policy Optimization (GRPO). GRPO can\n",
        "enhance your model's problem-solving skills on mathematical word problems,\n",
        "coding problems, etc.\n",
        "\n",
        "GRPO is an RL algorithm designed to enhance the reasoning abilities of LLMs. It\n",
        "is a variant of Proximal Policy Optimization (PPO) that reduces memory usage by\n",
        "eliminating the need for a separate value function model. GRPO works by\n",
        "generating multiple responses for a given prompt, evaluating these responses\n",
        "using a reward model, and then calculating a relative advantage based on the\n",
        "group's performance to update the policy.\n",
        "\n",
        "In this tutorial we use Colab's `v2-8` TPU. Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afofSj37qYz6",
      "metadata": {
        "id": "afofSj37qYz6"
      },
      "source": [
        "## Install necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "epROJO_sNvm2",
      "metadata": {
        "id": "epROJO_sNvm2"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y $(pip list --format=freeze | grep -i '^jax' | cut -d= -f1)\n",
        "!pip install jaxtyping\n",
        "!pip install jax[tpu]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z03GnyApTn1j",
      "metadata": {
        "id": "Z03GnyApTn1j"
      },
      "outputs": [],
      "source": [
        "!pip install -q kagglehub\n",
        "\n",
        "!pip install -q tensorflow\n",
        "!pip install -q tensorboardX\n",
        "!pip install -q grain\n",
        "!pip install -q git+https://github.com/google/tunix\n",
        "!pip install -q git+https://github.com/google/qwix\n",
        "\n",
        "!pip uninstall -q -y flax\n",
        "!pip install -q git+https://github.com/google/flax.git\n",
        "\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LnF9ZACiTn1k",
      "metadata": {
        "id": "LnF9ZACiTn1k"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "McTNo_r8Tn1k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McTNo_r8Tn1k",
        "outputId": "e67a0f70-911e-49fb-d003-30d0dea71d82"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import gc\n",
        "import os\n",
        "from pprint import pprint\n",
        "import re\n",
        "import time\n",
        "\n",
        "from flax import nnx\n",
        "import grain\n",
        "import humanize\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import kagglehub\n",
        "import optax\n",
        "from orbax import checkpoint as ocp\n",
        "import qwix\n",
        "import tensorflow_datasets as tfds\n",
        "from tqdm.auto import tqdm\n",
        "from tunix.generate import sampler as sampler_lib\n",
        "from tunix.models.gemma import data as data_lib\n",
        "from tunix.models.gemma import gemma as gemma_lib\n",
        "from tunix.models.gemma import params as params_lib\n",
        "from tunix.rl import rl_cluster as rl_cluster_lib\n",
        "from tunix.rl.rollout import base_rollout\n",
        "from tunix.rl.grpo.grpo_learner import GrpoConfig, GrpoLearner\n",
        "from tunix.sft import metrics_logger"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Eu_NI9nHTn1k",
      "metadata": {
        "id": "Eu_NI9nHTn1k"
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Let's define the configuration we are going to use. Note that this is by no\n",
        "means a \"perfect\" set of hyperparameters. To get good results, you might have\n",
        "to train the model for longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZPPKme47Tn1k",
      "metadata": {
        "id": "ZPPKme47Tn1k"
      },
      "outputs": [],
      "source": [
        "# ====== Data ======\n",
        "TRAIN_DATA_DIR = \"./data/train\"\n",
        "TEST_DATA_DIR = \"./data/test\"\n",
        "TRAIN_FRACTION = 1.0\n",
        "\n",
        "# ====== LoRA ======\n",
        "RANK = 64\n",
        "ALPHA = 64.0\n",
        "\n",
        "# ====== Sharding ======\n",
        "MESH = [(1, 4), (\"fsdp\", \"tp\")]\n",
        "\n",
        "# ====== GRPO ======\n",
        "# === Generation during GRPO training ===\n",
        "MAX_PROMPT_LENGTH = 256\n",
        "TOTAL_GENERATION_STEPS = 768\n",
        "# Important to keep a high-ish temperature for varied, diverse responses during\n",
        "# training.\n",
        "TEMPERATURE = 0.9\n",
        "TOP_P = 1.0\n",
        "TOP_K = 50\n",
        "# The number of times the policy generates multiple responses for a given prompt\n",
        "# within a single training step. This corresponds to `G` in Algorithm 1 in the\n",
        "# paper. The \"group\" in GRPO comes from here.\n",
        "NUM_GENERATIONS = 2\n",
        "\n",
        "# === other GRPO configs ===\n",
        "# The number of iterations per batch (ùúá in GRPO algo 1).\n",
        "NUM_ITERATIONS = 1\n",
        "# The coefficient for the KL divergence penalty (ùõΩ) in the GRPO loss function.\n",
        "# Important to keep a high enough value for this, otherwise, the KL divergence\n",
        "# can increase unchecked.\n",
        "BETA = 0.08\n",
        "# Epsilon value for clipping (ùúÄ in GRPO loss in paper). Similar to PPO, for\n",
        "# stable updates.\n",
        "EPSILON = 0.2\n",
        "\n",
        "# ====== Training ======\n",
        "BATCH_SIZE = 1\n",
        "# Increase `NUM_BATCHES` and `MAX_STEPS` for better results.\n",
        "NUM_BATCHES = 3738\n",
        "# Keep `NUM_TEST_BATCHES` low so that evaluation runs quickly. It can be\n",
        "# increased to a max. of 330 (if batch size is 4).\n",
        "NUM_TEST_BATCHES = 100\n",
        "\n",
        "EVAL_EVERY_N_STEPS = 10  # this doesn't matter if `TRAIN_FRACTION = 1.0`.\n",
        "NUM_EPOCHS = 1  # can potentially train for more epochs\n",
        "\n",
        "# Number of training steps.\n",
        "MAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)\n",
        "\n",
        "# === AdamW, warmup, cosine scheduler ===\n",
        "LEARNING_RATE = 3e-6\n",
        "B1 = 0.9\n",
        "B2 = 0.99\n",
        "WEIGHT_DECAY = 0.1\n",
        "# == Cosine decay with warmup scheduler ==\n",
        "# Linearly increase learning rate from 0. to 5e-6 in the first 10% training\n",
        "# steps, and then gradually decrease the learning rate to 0 using cosine\n",
        "# scheduler.\n",
        "WARMUP_STEPS = 0.1 * MAX_STEPS\n",
        "# == Grad clipping ==\n",
        "# Grad clipping to prevent large gradients. Found this\n",
        "# important to keep KL divergence in check.\n",
        "MAX_GRAD_NORM = 0.1\n",
        "\n",
        "# Checkpoint saving\n",
        "INTERMEDIATE_CKPT_DIR = \"/content/intermediate_ckpt/\"\n",
        "CKPT_DIR = \"/content/ckpts/\"\n",
        "SAVE_INTERVAL_STEPS = 500\n",
        "MAX_TO_KEEP = 4\n",
        "\n",
        "# ====== Inference ======\n",
        "GENERATION_CONFIGS = {\n",
        "    # greedy search\n",
        "    \"greedy\": {\"temperature\": 1e-4, \"top_k\": 1, \"top_p\": 1.0},\n",
        "    # some randomness\n",
        "    \"standard\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
        "    # liberal\n",
        "    \"liberal\": {\"temperature\": 0.85, \"top_k\": 2000, \"top_p\": 1.0},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ngjtE-63Tn1k",
      "metadata": {
        "id": "ngjtE-63Tn1k"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wjMFOr7aTn1k",
      "metadata": {
        "id": "wjMFOr7aTn1k"
      },
      "outputs": [],
      "source": [
        "def show_hbm_usage():\n",
        "  \"\"\"Displays memory usage per device.\"\"\"\n",
        "  fmt_size = functools.partial(humanize.naturalsize, binary=True)\n",
        "\n",
        "  for d in jax.local_devices():\n",
        "    stats = d.memory_stats()\n",
        "    used = stats[\"bytes_in_use\"]\n",
        "    limit = stats[\"bytes_limit\"]\n",
        "    print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:%}) on {d}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6BtpYMlaTn1k",
      "metadata": {
        "id": "6BtpYMlaTn1k"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "First, let's define some special tokens. We instruct the model to first reason\n",
        "between the `<reasoning>` and `</reasoning>` tokens. After\n",
        "reasoning, we expect it to provide the answer between the `<answer>` and\n",
        "`</answer>` tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h6RGv1kSTn1k",
      "metadata": {
        "id": "h6RGv1kSTn1k"
      },
      "outputs": [],
      "source": [
        "reasoning_start = \"<reasoning>\"\n",
        "reasoning_end = \"</reasoning>\"\n",
        "solution_start = \"<answer>\"\n",
        "solution_end = \"</answer>\"\n",
        "\n",
        "\n",
        "SYSTEM_PROMPT = f\"\"\"You are given a problem. Think about the problem and \\\n",
        "provide your reasoning. Place it between {reasoning_start} and \\\n",
        "{reasoning_end}. Then, provide the final answer (i.e., just one numerical \\\n",
        "value) between {solution_start} and {solution_end}.\"\"\"\n",
        "\n",
        "TEMPLATE = \"\"\"<start_of_turn>user\n",
        "{system_prompt}\n",
        "\n",
        "{question}<end_of_turn>\n",
        "<start_of_turn>model\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WASP9N5JTn1k",
      "metadata": {
        "id": "WASP9N5JTn1k"
      },
      "source": [
        "We use OpenAI's GSM8K dataset. GSM8K comprises grade school math word problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gTGjcSMNTn1k",
      "metadata": {
        "id": "gTGjcSMNTn1k"
      },
      "outputs": [],
      "source": [
        "def extract_hash_answer(text: str) -> str | None:\n",
        "  if \"####\" not in text:\n",
        "    return None\n",
        "  return text.split(\"####\")[1].strip()\n",
        "\n",
        "\n",
        "def get_dataset(data_dir, split=\"train\") -> grain.MapDataset:\n",
        "  # Download data\n",
        "  if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "  data = tfds.data_source(\n",
        "      \"gsm8k\",\n",
        "      split=split,\n",
        "      data_dir=data_dir,\n",
        "      builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n",
        "      download=True,\n",
        "  )\n",
        "\n",
        "  dataset = (\n",
        "      grain.MapDataset.source(data)\n",
        "      .shuffle(seed=42)\n",
        "      .map(\n",
        "          lambda x: {\n",
        "              # passed to model forward pass\n",
        "              \"prompts\": TEMPLATE.format(\n",
        "                  system_prompt=SYSTEM_PROMPT,\n",
        "                  question=x[\"question\"].decode(\"utf-8\"),\n",
        "              ),\n",
        "              # passed to reward functions\n",
        "              \"question\": x[\"question\"].decode(\"utf-8\"),\n",
        "              # passed to reward functions\n",
        "              \"answer\": extract_hash_answer(x[\"answer\"].decode(\"utf-8\")),\n",
        "          }\n",
        "      )\n",
        "  )\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KXhOL6GyTn1k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXhOL6GyTn1k",
        "outputId": "f631965e-e2d0-4742-eeec-4664330580c1"
      },
      "outputs": [],
      "source": [
        "dataset = get_dataset(TRAIN_DATA_DIR, \"train\").batch(BATCH_SIZE)[:NUM_BATCHES]\n",
        "\n",
        "if TRAIN_FRACTION == 1.0:\n",
        "  train_dataset = dataset.repeat(NUM_EPOCHS)\n",
        "  val_dataset = None\n",
        "else:\n",
        "  train_dataset = dataset[: int(len(dataset) * TRAIN_FRACTION)]\n",
        "  train_dataset = train_dataset.repeat(NUM_EPOCHS)\n",
        "\n",
        "  val_dataset = dataset[int(len(dataset) * TRAIN_FRACTION) :].repeat(NUM_EPOCHS)\n",
        "\n",
        "test_dataset = get_dataset(TEST_DATA_DIR, \"test\").batch(BATCH_SIZE)[\n",
        "    :NUM_TEST_BATCHES\n",
        "]\n",
        "\n",
        "len(train_dataset), len(val_dataset) if val_dataset is not None else 0, len(\n",
        "    test_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k7n8L0VzTn1k",
      "metadata": {
        "id": "k7n8L0VzTn1k"
      },
      "source": [
        "Let's see how one batch of the dataset looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5TF-wNQ2Tn1k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TF-wNQ2Tn1k",
        "outputId": "f5d183c8-a1d9-4ad5-d021-f8f151c3ab9a"
      },
      "outputs": [],
      "source": [
        "for ele in train_dataset[:1]:\n",
        "  pprint(ele)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BZxBR7Y_Tn1k",
      "metadata": {
        "id": "BZxBR7Y_Tn1k"
      },
      "source": [
        "## Load the policy model and the reference model\n",
        "\n",
        "The policy model is the model which is actually trained and whose weights are\n",
        "updated. The reference model is the model with which we compute KL divergence.\n",
        "This is to ensure that the policy updates are not huge and that it does not\n",
        "deviate too much from the reference model.\n",
        "\n",
        "Typically, the reference model is the base model, and the policy model is the\n",
        "same base model, but with LoRA parameters. Only the LoRA parameters are updated.\n",
        "\n",
        "Note: We perform full precision (fp32) training. You can, however, leverage\n",
        "Qwix for QAT.\n",
        "\n",
        "To load the model, you need to be on [Kaggle](https://www.kaggle.com/) and need\n",
        "to have agreed to the Gemma license\n",
        "[here](https://www.kaggle.com/models/google/gemma/flax/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E0a4AREIQ0kV",
      "metadata": {
        "id": "E0a4AREIQ0kV"
      },
      "outputs": [],
      "source": [
        "import kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thp6hhqfTn1k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "4ff690faa0d54ba9a909f81e4c139a90",
            "087bf97b79d54df4aa09d7ba3cd8f4c5",
            "721036fb70b24771910ea849772ac4a8",
            "5fcfbdc1ee5d447390cb581f265e4531",
            "c87d19692bcb4773b2c73143b7961ad5",
            "791ff3bcfad5435a8c26d951239a8f82",
            "40700d18f9fe4912ad67835cc38ca302",
            "f5ca1c4bc34944938289957ab67521be",
            "06e56eb8151244be9ad07cba0876e657",
            "541daa35f17f445aa12b515e363b6052",
            "c8eb068de4f3468bb14e4bcafb81ccc8",
            "33d31b86a2c54a7cba282b8d8ba58863",
            "30ba8d02169c4e0aaeac1ad7c551b12b",
            "c49fd462620b430c823d26ee5e35451f",
            "5d84d606cbbd44a483ad2243f68c43d2",
            "f6f4c3bd79dd45b2815dd44d783fb034",
            "ea39277bc7f64e788af63074ce27938f",
            "e91e3f489834436c83a2ff44f0f52560",
            "1b2bbe3f4c8e4379bbaece8c0b4e7255",
            "3164117a82ce46d69d8bf3381ee57170",
            "f95ea66851b7484884b16a111fe948ed",
            "31212847ea574d7aac892fb7aca7f1b3",
            "ecfe2bb7e1d240aebd3d31c438b62946"
          ]
        },
        "id": "thp6hhqfTn1k",
        "outputId": "f4ca950d-3477-42c0-c563-b20157941051"
      },
      "outputs": [],
      "source": [
        "# Log in\n",
        "if \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n",
        "  kagglehub.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "srH2s_jzTn1k",
      "metadata": {
        "id": "srH2s_jzTn1k"
      },
      "outputs": [],
      "source": [
        "kaggle_ckpt_path = kagglehub.model_download(\"google/gemma-2/flax/gemma2-2b-it\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e72hWdAtaBc1",
      "metadata": {
        "id": "e72hWdAtaBc1"
      },
      "source": [
        "This code is a workaround to re-save the pre-trained model checkpoint from Kaggle into a local format that is compatible with the NNX library. The checkpoints on Kaggle don't work with NNX. So, we load the model, save the checkpoint locally, and then reload the model(sharded)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cIFAxgVOTn1k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIFAxgVOTn1k",
        "outputId": "58952f8d-b921-4776-b415-be7f82bcbd17"
      },
      "outputs": [],
      "source": [
        "params = params_lib.load_and_format_params(\n",
        "    os.path.join(kaggle_ckpt_path, \"gemma2-2b-it\")\n",
        ")\n",
        "gemma = gemma_lib.Transformer.from_params(params, version=\"2-2b-it\")\n",
        "checkpointer = ocp.StandardCheckpointer()\n",
        "_, state = nnx.split(gemma)\n",
        "# force=True parameter ensures that the new file will overwrite any old files that might already exist.\n",
        "checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state, force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JSz-XmQpTn1k",
      "metadata": {
        "id": "JSz-XmQpTn1k"
      },
      "outputs": [],
      "source": [
        "# Wait for the ckpt to save successfully.\n",
        "time.sleep(60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_w8kav8sTn1k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_w8kav8sTn1k",
        "outputId": "bb230181-0581-4962-8674-fc7938931173"
      },
      "outputs": [],
      "source": [
        "# Delete the intermediate model to save memory.\n",
        "del params\n",
        "del gemma\n",
        "del state\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tMUYWDY2blFR",
      "metadata": {
        "id": "tMUYWDY2blFR"
      },
      "source": [
        "### Model Loading and LoRA Application\n",
        "\n",
        "These two functions work together to load a base model from a checkpoint and apply a LoRA (Low-Rank Adaptation) layer to it.\n",
        "\n",
        "* `get_ref_model`: Loads the complete Gemma model from a specified checkpoint path. It uses **JAX sharding** to efficiently distribute the model parameters across multiple devices.\n",
        "* `get_lora_model`: Takes the base model and applies LoRA layers to it. It uses a `LoraProvider` to select specific layers (like attention and MLP layers) to be adapted. The resulting LoRA-infused model is then sharded and updated to ensure it's ready for distributed training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m2KD-nmbTn1k",
      "metadata": {
        "id": "m2KD-nmbTn1k"
      },
      "outputs": [],
      "source": [
        "def get_ref_model(ckpt_path):\n",
        "  mesh = jax.make_mesh(*MESH)\n",
        "  model_config = gemma_lib.TransformerConfig.gemma2_2b()\n",
        "  abs_gemma: nnx.Module = nnx.eval_shape(\n",
        "      lambda: gemma_lib.Transformer(model_config, rngs=nnx.Rngs(params=0))\n",
        "  )\n",
        "  abs_state = nnx.state(abs_gemma)\n",
        "  abs_state = jax.tree.map(\n",
        "      lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.float32, sharding=s),\n",
        "      abs_state,\n",
        "      nnx.get_named_sharding(abs_state, mesh),\n",
        "  )\n",
        "  checkpointer = ocp.StandardCheckpointer()\n",
        "  restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n",
        "\n",
        "  graph_def, _ = nnx.split(abs_gemma)\n",
        "  gemma = nnx.merge(graph_def, restored_params)\n",
        "  return gemma, mesh, model_config\n",
        "\n",
        "\n",
        "def get_lora_model(base_model, mesh):\n",
        "  lora_provider = qwix.LoraProvider(\n",
        "      module_path=(\n",
        "          \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
        "          \".*attn_vec_einsum\"\n",
        "      ),\n",
        "      rank=RANK,\n",
        "      alpha=ALPHA,\n",
        "  )\n",
        "\n",
        "  model_input = base_model.get_model_input()\n",
        "  lora_model = qwix.apply_lora_to_model(\n",
        "      base_model, lora_provider, **model_input\n",
        "  )\n",
        "\n",
        "  with mesh:\n",
        "    state = nnx.state(lora_model)\n",
        "    pspecs = nnx.get_partition_spec(state)\n",
        "    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
        "    nnx.update(lora_model, sharded_state)\n",
        "\n",
        "  return lora_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-d8G_D6kbzzr",
      "metadata": {
        "id": "-d8G_D6kbzzr"
      },
      "source": [
        "Now we load a pre-trained Gemma model using the JAX NNX library and displays its structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kSdZ7aGhTn1k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "kSdZ7aGhTn1k",
        "outputId": "c087d623-aa53-4bb9-ef7e-bbf8c12695df"
      },
      "outputs": [],
      "source": [
        "# Reference model\n",
        "gemma, mesh, model_config = get_ref_model(\n",
        "    ckpt_path=os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\n",
        ")\n",
        "nnx.display(gemma)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MrUNQjNfcZvR",
      "metadata": {
        "id": "MrUNQjNfcZvR"
      },
      "source": [
        "This code applies a LoRA (Low-Rank Adaptation) layer to the base Gemma model and then displays the new model's structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4i3CfJ1gTn1k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "4i3CfJ1gTn1k",
        "outputId": "5f8d2879-4b21-4f48-cc23-18fded834254"
      },
      "outputs": [],
      "source": [
        "# Policy model\n",
        "lora_gemma = get_lora_model(gemma, mesh=mesh)\n",
        "nnx.display(lora_gemma)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zLzR1tJfTn1k",
      "metadata": {
        "id": "zLzR1tJfTn1k"
      },
      "source": [
        "## Define reward functions\n",
        "\n",
        "We define four reward functions:\n",
        "\n",
        "- reward if the format of the output exactly matches the instruction given in\n",
        "`TEMPLATE`;\n",
        "- reward if the format of the output approximately matches the instruction given\n",
        "in `TEMPLATE`;\n",
        "- reward if the answer is correct/partially correct;\n",
        "- Sometimes, the text between `<answer>`, `</answer>` might not be one\n",
        "  number. So, extract the number, and reward the model if the answer is correct.\n",
        "\n",
        "The reward functions are inspired from\n",
        "[here](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb).\n",
        "\n",
        "First off, let's define a RegEx for checking whether the format matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C7Beft8wTn1k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7Beft8wTn1k",
        "outputId": "e32504ac-1da5-416c-ae17-f6f871c06a7a"
      },
      "outputs": [],
      "source": [
        "match_format = re.compile(\n",
        "    rf\"^[\\s]{{0,}}\"\n",
        "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\n",
        "    rf\"{solution_start}(.+?){solution_end}\"\n",
        "    rf\"[\\s]{{0,}}$\",\n",
        "    flags=re.MULTILINE | re.DOTALL,\n",
        ")\n",
        "\n",
        "match_format.search(\n",
        "    f\"{reasoning_start}Let me\"\n",
        "    f\" think!{reasoning_end}{solution_start}2{solution_end}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fe1rF15zTn1k",
      "metadata": {
        "id": "Fe1rF15zTn1k"
      },
      "source": [
        "Give the model a reward of 3 points if the format matches exactly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_fhQ6pY2Tn1k",
      "metadata": {
        "id": "_fhQ6pY2Tn1k"
      },
      "outputs": [],
      "source": [
        "def match_format_exactly(prompts, completions, **kargs):\n",
        "  scores = []\n",
        "  for completion in completions:\n",
        "    score = 0\n",
        "    response = completion\n",
        "    # Match if format is seen exactly!\n",
        "    if match_format.search(response) is not None:\n",
        "      score += 3.0\n",
        "    scores.append(score)\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sWdAdUHuTn1k",
      "metadata": {
        "id": "sWdAdUHuTn1k"
      },
      "source": [
        "We also reward the model if the format of the output matches partially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uOhO4f3-Tn1k",
      "metadata": {
        "id": "uOhO4f3-Tn1k"
      },
      "outputs": [],
      "source": [
        "def match_format_approximately(prompts, completions, **kargs):\n",
        "  scores = []\n",
        "\n",
        "  for completion in completions:\n",
        "    score = 0\n",
        "    response = completion\n",
        "    # Count how many keywords are seen - we penalize if too many!\n",
        "    # If we see 1, then plus some points!\n",
        "    score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n",
        "    score += 0.5 if response.count(reasoning_end) == 1 else -0.5\n",
        "    score += 0.5 if response.count(solution_start) == 1 else -0.5\n",
        "    score += 0.5 if response.count(solution_end) == 1 else -0.5\n",
        "    scores.append(score)\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A2fNZDgTTn1k",
      "metadata": {
        "id": "A2fNZDgTTn1k"
      },
      "source": [
        "Reward the model if the answer is correct. A reward is also given if the answer\n",
        "does not match exactly, i.e., based on how close the answer is to the correct\n",
        "value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S8zcWsmhTn1k",
      "metadata": {
        "id": "S8zcWsmhTn1k"
      },
      "outputs": [],
      "source": [
        "def check_answer(prompts, completions, answer, **kargs):\n",
        "  responses = completions\n",
        "\n",
        "  extracted_responses = [\n",
        "      guess.group(1) if (guess := match_format.search(r)) is not None else None\n",
        "      for r in responses\n",
        "  ]\n",
        "\n",
        "  scores = []\n",
        "  for guess, true_answer in zip(extracted_responses, answer):\n",
        "    score = 0\n",
        "    if guess is None:\n",
        "      scores.append(0)\n",
        "      continue\n",
        "    # Correct answer gets 3 points!\n",
        "    if guess == true_answer:\n",
        "      score += 3.0\n",
        "    # Match if spaces are seen\n",
        "    elif guess.strip() == true_answer.strip():\n",
        "      score += 1.5\n",
        "    else:\n",
        "      # We also reward it if the answer is close via ratios!\n",
        "      # Ie if the answer is within some range, reward it!\n",
        "      try:\n",
        "        ratio = float(guess) / float(true_answer)\n",
        "        if ratio >= 0.9 and ratio <= 1.1:\n",
        "          score += 0.5\n",
        "        elif ratio >= 0.8 and ratio <= 1.2:\n",
        "          score += 0.25\n",
        "        else:\n",
        "          score -= 1.0  # Penalize wrong answers\n",
        "      except:\n",
        "        score -= 0.5  # Penalize\n",
        "    scores.append(score)\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nIpOVv78Tn1k",
      "metadata": {
        "id": "nIpOVv78Tn1k"
      },
      "source": [
        "Sometimes, the text between `<answer>` and `</answer>` might not be one\n",
        "number; it can be a sentence. So, we extract the number and compare the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NXvRtbk8Tn1k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXvRtbk8Tn1k",
        "outputId": "2826d590-d90c-4893-b6aa-ca90d79df8c7"
      },
      "outputs": [],
      "source": [
        "match_numbers = re.compile(\n",
        "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\", flags=re.MULTILINE | re.DOTALL\n",
        ")\n",
        "match_numbers.findall(f\"{solution_start}  0.34  {solution_end}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oxZQAFKOTn1k",
      "metadata": {
        "id": "oxZQAFKOTn1k"
      },
      "outputs": [],
      "source": [
        "def check_numbers(prompts, completions, answer, **kargs):\n",
        "  question = kargs[\"question\"]\n",
        "  responses = completions\n",
        "\n",
        "  extracted_responses = [\n",
        "      guess.group(1) if (guess := match_numbers.search(r)) is not None else None\n",
        "      for r in responses\n",
        "  ]\n",
        "\n",
        "  scores = []\n",
        "  print(\"START ============================\")\n",
        "  print(f\"Question: {question[0]}\")\n",
        "  print(f\"Answer: {answer[0]}\")\n",
        "  print(f\"Response: {responses[0]}\")\n",
        "  print(f\"Extracted: {extracted_responses[0]}\")\n",
        "  print(\"END ==============================\")\n",
        "  for guess, true_answer in zip(extracted_responses, answer):\n",
        "    if guess is None:\n",
        "      scores.append(0)\n",
        "      continue\n",
        "    # Convert to numbers\n",
        "    try:\n",
        "      true_answer = float(true_answer.strip())\n",
        "      guess = float(guess.strip())\n",
        "      scores.append(1.5 if guess == true_answer else 0.0)\n",
        "    except:\n",
        "      scores.append(0)\n",
        "      continue\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AaiYMJxFTn1k",
      "metadata": {
        "id": "AaiYMJxFTn1k"
      },
      "source": [
        "## Evaluate\n",
        "\n",
        "\n",
        "Before we train the model, let's evaluate the model on the test set so we can\n",
        "see the improvement post training.\n",
        "\n",
        "We evaluate it in two ways:\n",
        "\n",
        "**Quantitative**\n",
        "\n",
        "* **Answer Accuracy**: percentage of samples for which the model predicts the\n",
        "correct final numerical answer  \n",
        "* **Answer (Partial) Accuracy**: percentage of samples for which the model\n",
        "predicts a final numerical answer such that the \\`model answer / answer\\`\n",
        "ratio lies between 0.9 and 1.1.  \n",
        "* **Format Accuracy**: percentage of samples for which the model outputs the\n",
        "correct format, i.e., reasoning between the reasoning special tokens, and the\n",
        "final answer between the \\`\\<start\\_answer\\>\\`, \\`\\<end\\_answer\\>\\` tokens.\n",
        "\n",
        "**Qualitative**\n",
        "\n",
        "We'll also print outputs for a few given questions so that we can compare the generated output later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_k58bOicUHJy",
      "metadata": {
        "id": "_k58bOicUHJy"
      },
      "outputs": [],
      "source": [
        "def generate(\n",
        "    question, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None\n",
        "):\n",
        "  \"\"\"Given prompt, generates text.\"\"\"\n",
        "\n",
        "  if isinstance(question, str):\n",
        "    input_batch = [\n",
        "        TEMPLATE.format(\n",
        "            system_prompt=SYSTEM_PROMPT,\n",
        "            question=question,\n",
        "        ),\n",
        "    ]\n",
        "  else:\n",
        "    input_batch = [\n",
        "        TEMPLATE.format(\n",
        "            system_prompt=SYSTEM_PROMPT,\n",
        "            question=q,\n",
        "        )\n",
        "        for q in question\n",
        "    ]\n",
        "\n",
        "  out_data = sampler(\n",
        "      input_strings=input_batch,\n",
        "      total_generation_steps=768,\n",
        "      temperature=temperature,\n",
        "      top_k=top_k,\n",
        "      top_p=top_p,\n",
        "      echo=False,\n",
        "      seed=seed if seed is not None else None,\n",
        "  )\n",
        "\n",
        "  output = out_data.text\n",
        "  if isinstance(question, str):\n",
        "    return output[0]\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yJo2nuKB-wlw",
      "metadata": {
        "id": "yJo2nuKB-wlw"
      },
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    dataset,\n",
        "    sampler,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    num_passes=1,\n",
        "    corr_lst=False,\n",
        "    make_lst=False,\n",
        "):\n",
        "  \"\"\"Computes accuracy and percentage of outputs matching the format.\"\"\"\n",
        "\n",
        "  response_lst = []\n",
        "  corr = 0\n",
        "  partially_corr = 0\n",
        "  corr_format = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch in tqdm(dataset):\n",
        "    answers = batch[\"answer\"]\n",
        "    questions = batch[\"question\"]\n",
        "\n",
        "    multiple_call_responses = [[] for _ in range(len(questions))]\n",
        "    for p in range(num_passes):\n",
        "      responses = generate(\n",
        "          questions, sampler, temperature, top_k, top_p, seed=p\n",
        "      )\n",
        "      for idx, response in enumerate(responses):\n",
        "        multiple_call_responses[idx].append(response)\n",
        "\n",
        "    for question, multiple_call_response, answer in zip(\n",
        "        questions, multiple_call_responses, answers\n",
        "    ):\n",
        "      # check answer\n",
        "      corr_ctr_per_question = 0\n",
        "      partially_corr_per_question = 0\n",
        "      corr_format_per_question = 0\n",
        "      for response in multiple_call_response:\n",
        "        extracted_response = (\n",
        "            guess.group(1)\n",
        "            if (guess := match_numbers.search(response)) is not None\n",
        "            else \"-1000000\"\n",
        "        )\n",
        "        try:\n",
        "          if float(extracted_response.strip()) == float(answer.strip()):\n",
        "            corr_ctr_per_question += 1\n",
        "\n",
        "          ratio = float(extracted_response.strip()) / float(answer.strip())\n",
        "          if ratio >= 0.9 and ratio <= 1.1:\n",
        "            partially_corr_per_question += 1\n",
        "        except:\n",
        "          print(\"SKIPPED\")\n",
        "\n",
        "        # check format\n",
        "        if match_format.search(response) is not None:\n",
        "          corr_format_per_question += 1\n",
        "\n",
        "        if (\n",
        "            corr_ctr_per_question > 0\n",
        "            and partially_corr_per_question > 0\n",
        "            and corr_format_per_question > 0\n",
        "        ):\n",
        "          break\n",
        "\n",
        "      if corr_ctr_per_question > 0:\n",
        "        corr += 1\n",
        "        if corr_lst and make_lst:\n",
        "          response_lst.append((question, answer, multiple_call_response))\n",
        "      else:\n",
        "        if not corr_lst and make_lst:\n",
        "          response_lst.append((question, answer, multiple_call_response))\n",
        "      if partially_corr_per_question > 0:\n",
        "        partially_corr += 1\n",
        "      if corr_format_per_question > 0:\n",
        "        corr_format += 1\n",
        "\n",
        "      total += 1\n",
        "      if total % 10 == 0:\n",
        "        print(\n",
        "            f\"===> {corr=}, {total=}, {corr / total * 100=}, \"\n",
        "            f\"{partially_corr / total * 100=}, {corr_format / total * 100=}\"\n",
        "        )\n",
        "\n",
        "  to_return = (\n",
        "      corr,\n",
        "      total,\n",
        "      corr / total * 100,\n",
        "      partially_corr / total * 100,\n",
        "      corr_format / total * 100,\n",
        "  )\n",
        "  if make_lst:\n",
        "    return to_return, response_lst\n",
        "  return to_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HZMO-KflTn1k",
      "metadata": {
        "id": "HZMO-KflTn1k"
      },
      "outputs": [],
      "source": [
        "gemma_tokenizer = data_lib.GemmaTokenizer()\n",
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=lora_gemma,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YQM-tzXWUmoE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257,
          "referenced_widgets": [
            "d03f6fd88bb04368b8e9075e1c6e19f3",
            "676c8d61c501433f8e241c8a49d224d9",
            "23fea0d29c744576a978ddb8bb70e192",
            "753c9e6740094d378235a36ffba85498",
            "7dbd4ffb3d6a4652a35606ab920bc243",
            "8eefd13762564bf39d0f15105bdf45c3",
            "9dfe6527e59b4d7c9c9d19ea6d2adf3a",
            "a76d11b7c17046288b2d32b7f945fbfe",
            "6b55ca6beb5a48f49ad94645543b7085",
            "c89885d0e6d54f4ab7c8212ad2b3e3b0",
            "7df4812ff202410a9f862247d173fe18"
          ]
        },
        "id": "YQM-tzXWUmoE",
        "outputId": "29d22c2e-f5b6-4679-ae8c-303f824d927e"
      },
      "outputs": [],
      "source": [
        "(corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n",
        "    test_dataset,\n",
        "    sampler,\n",
        "    **GENERATION_CONFIGS[\"greedy\"],\n",
        ")\n",
        "print(\n",
        "    f\"{corr=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\"\n",
        "    f\" {format_accuracy=}%\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PJV6wNGY-3PG",
      "metadata": {
        "id": "PJV6wNGY-3PG"
      },
      "outputs": [],
      "source": [
        "# for eval_example in QUALITATIVE_EVAL_EXAMPLES:\n",
        "#   question = eval_example[\"question\"]\n",
        "#   answer = eval_example[\"answer\"]\n",
        "#   response = generate(\n",
        "#       question,\n",
        "#       sampler,\n",
        "#       temperature=INFERENCE_TEMPERATURE,\n",
        "#       top_k=INFERENCE_TOP_K,\n",
        "#       top_p=INFERENCE_TOP_P,\n",
        "#   )\n",
        "\n",
        "#   print(f\"Question:\\n{question}\")\n",
        "#   print(f\"Answer:\\n{answer}\")\n",
        "#   print(f\"Response:\\n{response}\")\n",
        "#   print(\"===============\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-CmB2ZT9Tn1l",
      "metadata": {
        "id": "-CmB2ZT9Tn1l"
      },
      "source": [
        "## Train\n",
        "\n",
        "Let's set up all the configs first - checkpointing, metric logging and training.\n",
        "We then train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mHzdsYsGTn1l",
      "metadata": {
        "id": "mHzdsYsGTn1l"
      },
      "outputs": [],
      "source": [
        "# Ckpt saving\n",
        "checkpointing_options = ocp.CheckpointManagerOptions(\n",
        "    save_interval_steps=SAVE_INTERVAL_STEPS, max_to_keep=MAX_TO_KEEP\n",
        ")\n",
        "\n",
        "# Metrics logger\n",
        "metrics_logging_options = metrics_logger.MetricsLoggerOptions(\n",
        "    log_dir=\"/content/tmp/tensorboard/grpo_demo\", flush_every_n_steps=20\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u1Sc1fNC_CJ7",
      "metadata": {
        "id": "u1Sc1fNC_CJ7"
      },
      "outputs": [],
      "source": [
        "# Logs\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/tmp/tensorboard/grpo_demo --port=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YWvBkWBsruom",
      "metadata": {
        "id": "YWvBkWBsruom"
      },
      "outputs": [],
      "source": [
        "# Optimizer, learning rate scheduler, gradient clipping\n",
        "optimizer = optax.adamw(\n",
        "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
        "        init_value=0.0,\n",
        "        peak_value=LEARNING_RATE,\n",
        "        warmup_steps=WARMUP_STEPS,\n",
        "        decay_steps=MAX_STEPS,\n",
        "        end_value=0.0,\n",
        "    ),\n",
        "    b1=B1,\n",
        "    b2=B2,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        ")\n",
        "if MAX_GRAD_NORM is not None:\n",
        "  optimizer = optax.chain(\n",
        "      optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n",
        "      optimizer,\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_6VxFW1ZTn1l",
      "metadata": {
        "id": "_6VxFW1ZTn1l"
      },
      "outputs": [],
      "source": [
        "# Training config\n",
        "cluster_config = rl_cluster_lib.ClusterConfig(\n",
        "    role_to_mesh={\n",
        "        rl_cluster_lib.Role.ACTOR: mesh,\n",
        "        rl_cluster_lib.Role.REFERENCE: mesh,\n",
        "        rl_cluster_lib.Role.ROLLOUT: mesh,\n",
        "    },\n",
        "    rollout_engine='vanilla',\n",
        "    offload_to_cpu=False,\n",
        "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
        "        actor_optimizer=optimizer,\n",
        "        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "        max_steps=MAX_STEPS,\n",
        "        gradient_accumulation_steps=1,\n",
        "        # metrics logging\n",
        "        metrics_logging_options=metrics_logging_options,\n",
        "        # checkpoint saving\n",
        "        checkpoint_root_directory=CKPT_DIR,\n",
        "        checkpointing_options=checkpointing_options,\n",
        "    ),\n",
        "    rollout_config=base_rollout.RolloutConfig(\n",
        "        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n",
        "        max_prompt_length=MAX_PROMPT_LENGTH,\n",
        "        kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_p=TOP_P,\n",
        "        top_k=TOP_K,\n",
        "    ),\n",
        ")\n",
        "\n",
        "grpo_config = GrpoConfig(\n",
        "    num_generations=NUM_GENERATIONS,\n",
        "    num_iterations=NUM_ITERATIONS,\n",
        "    beta=BETA,\n",
        "    epsilon=EPSILON,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GEKbUOU6fMuR",
      "metadata": {
        "id": "GEKbUOU6fMuR"
      },
      "source": [
        "### Setting Up the GRPO Trainer\n",
        "This code initializes a system for training. It first creates an `RLCluster` instance, which brings together the **policy model (`actor`)**, a **reference model (`reference`)**, and a **tokenizer**. The `actor` is a trainable LoRA model, while the `reference` is a fixed base model used to guide training. It then creates a `GrpoLearner`, a specialized trainer that uses a list of **reward functions** to evaluate and optimize the model's output, completing the RL training setup.\n",
        "\n",
        "**To get started, you may need to choose how you want to use Weights & Biases:**\n",
        "\n",
        "**Option 1 (Type 1)**: If you're running a quick experiment or just testing things out, choose this. It creates a temporary, private dashboard right in your browser without requiring you to log in or create an account.\n",
        "\n",
        "**Option 2 (Type 2)**: If you have an existing W&B account and want to save your project's history to your personal dashboard, choose this. You'll be prompted to enter your API key or log in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OIe1lO08Tn1l",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "OIe1lO08Tn1l",
        "outputId": "c2471bc4-b1bb-483e-da40-bfb55cd62c51"
      },
      "outputs": [],
      "source": [
        "# RL cluster\n",
        "rl_cluster = rl_cluster_lib.RLCluster(\n",
        "    actor=lora_gemma,\n",
        "    reference=gemma,\n",
        "    tokenizer=data_lib.GemmaTokenizer(),\n",
        "    cluster_config=cluster_config,\n",
        ")\n",
        "\n",
        "# GRPO Trainer\n",
        "grpo_trainer = GrpoLearner(\n",
        "    rl_cluster=rl_cluster,\n",
        "    reward_fns=[\n",
        "        match_format_exactly,\n",
        "        match_format_approximately,\n",
        "        check_answer,\n",
        "        check_numbers,\n",
        "    ],\n",
        "    grpo_config=grpo_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IrCGNcRHgZul",
      "metadata": {
        "id": "IrCGNcRHgZul"
      },
      "source": [
        "Now let's start the training steps!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S27XDebYTn1l",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5028a502eb9b4a4685bacd9c3334a6e6",
            "c13c7f0891574c0590666d423e340564",
            "5ad2fc83f4c840158f131b48e150725d",
            "1ec8eef707514c449547500d632212c8",
            "7667454a4a824f56b9f65c74d1971005",
            "bbbf27c10bc9428d82cc9a432af6cfea",
            "75bc761f1c7e4121b222b9d290ef042a",
            "a02cab6068c6402aa716d3ed4b1d8626",
            "9e4288fa92fa4ca0a90f5027915409bf",
            "9c30773ff8fe465aba27c94548f35844",
            "7eac2c97629e4bedb14a06000d924a36"
          ]
        },
        "id": "S27XDebYTn1l",
        "outputId": "c707b00f-3610-4cb6-fbcb-09f9792f2f3f"
      },
      "outputs": [],
      "source": [
        "with mesh:\n",
        "  grpo_trainer.train(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FzIP8glkTn1l",
      "metadata": {
        "id": "FzIP8glkTn1l"
      },
      "source": [
        "## Evaluate\n",
        "\n",
        "Let's evaluate our model after the above training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V-73HfP1Tn1l",
      "metadata": {
        "id": "V-73HfP1Tn1l"
      },
      "outputs": [],
      "source": [
        "# Load checkpoint first.\n",
        "\n",
        "trained_ckpt_path = os.path.join(CKPT_DIR, str(MAX_STEPS), \"model_params\")\n",
        "\n",
        "abs_params = jax.tree.map(\n",
        "    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
        "    nnx.state(lora_gemma, nnx.LoRAParam),\n",
        ")\n",
        "checkpointer = ocp.StandardCheckpointer()\n",
        "trained_lora_params = checkpointer.restore(trained_ckpt_path, target=abs_params)\n",
        "\n",
        "nnx.update(\n",
        "    lora_gemma,\n",
        "    jax.tree.map(\n",
        "        lambda a, b: b,\n",
        "        nnx.state(lora_gemma, nnx.LoRAParam),\n",
        "        trained_lora_params,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1vY9kl-ITn1l",
      "metadata": {
        "id": "1vY9kl-ITn1l"
      },
      "outputs": [],
      "source": [
        "gemma_tokenizer = data_lib.GemmaTokenizer()\n",
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=lora_gemma,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nz0q_gGHqYz6",
      "metadata": {
        "id": "nz0q_gGHqYz6"
      },
      "outputs": [],
      "source": [
        "(corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n",
        "    test_dataset,\n",
        "    sampler,\n",
        "    **GENERATION_CONFIGS[\"greedy\"],\n",
        ")\n",
        "print(\n",
        "    f\"{corr=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\"\n",
        "    f\" {format_accuracy=}%\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jd9gpYVpUd3_",
      "metadata": {
        "id": "Jd9gpYVpUd3_"
      },
      "outputs": [],
      "source": [
        "# for eval_example in QUALITATIVE_EVAL_EXAMPLES:\n",
        "#   question = eval_example[\"question\"]\n",
        "#   answer = eval_example[\"answer\"]\n",
        "#   response = generate(\n",
        "#       question,\n",
        "#       sampler,\n",
        "#       temperature=INFERENCE_TEMPERATURE,\n",
        "#       top_k=INFERENCE_TOP_K,\n",
        "#       top_p=INFERENCE_TOP_P,\n",
        "#   )\n",
        "\n",
        "#   print(f\"Question:\\n{question}\")\n",
        "#   print(f\"Answer:\\n{answer}\")\n",
        "#   print(f\"Response:\\n{response}\")\n",
        "#   print(\"===============\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RrE_Rvaz_8CV",
      "metadata": {
        "id": "RrE_Rvaz_8CV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06e56eb8151244be9ad07cba0876e657": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "087bf97b79d54df4aa09d7ba3cd8f4c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5ca1c4bc34944938289957ab67521be",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_06e56eb8151244be9ad07cba0876e657",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "1b2bbe3f4c8e4379bbaece8c0b4e7255": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ec8eef707514c449547500d632212c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c30773ff8fe465aba27c94548f35844",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7eac2c97629e4bedb14a06000d924a36",
            "value": "‚Äá1102/3738‚Äá[21:33&lt;1:37:57,‚Äá‚Äá2.23s/step,‚Äátrain_loss=-0.015,‚Äátrain_perplexity=0.985,‚Äátrain_tflops=0.396,‚Äátrain_rewards/overall=6.05,‚Äátrain_kl=0.004]"
          }
        },
        "23fea0d29c744576a978ddb8bb70e192": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a76d11b7c17046288b2d32b7f945fbfe",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b55ca6beb5a48f49ad94645543b7085",
            "value": 100
          }
        },
        "30ba8d02169c4e0aaeac1ad7c551b12b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31212847ea574d7aac892fb7aca7f1b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3164117a82ce46d69d8bf3381ee57170": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33d31b86a2c54a7cba282b8d8ba58863": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40700d18f9fe4912ad67835cc38ca302": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "4ff690faa0d54ba9a909f81e4c139a90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f95ea66851b7484884b16a111fe948ed"
            ],
            "layout": "IPY_MODEL_40700d18f9fe4912ad67835cc38ca302"
          }
        },
        "5028a502eb9b4a4685bacd9c3334a6e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c13c7f0891574c0590666d423e340564",
              "IPY_MODEL_5ad2fc83f4c840158f131b48e150725d",
              "IPY_MODEL_1ec8eef707514c449547500d632212c8"
            ],
            "layout": "IPY_MODEL_7667454a4a824f56b9f65c74d1971005"
          }
        },
        "541daa35f17f445aa12b515e363b6052": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ad2fc83f4c840158f131b48e150725d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a02cab6068c6402aa716d3ed4b1d8626",
            "max": 3738,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e4288fa92fa4ca0a90f5027915409bf",
            "value": 1102
          }
        },
        "5d84d606cbbd44a483ad2243f68c43d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "5fcfbdc1ee5d447390cb581f265e4531": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_33d31b86a2c54a7cba282b8d8ba58863",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_30ba8d02169c4e0aaeac1ad7c551b12b",
            "value": ""
          }
        },
        "676c8d61c501433f8e241c8a49d224d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8eefd13762564bf39d0f15105bdf45c3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9dfe6527e59b4d7c9c9d19ea6d2adf3a",
            "value": "100%"
          }
        },
        "6b55ca6beb5a48f49ad94645543b7085": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "721036fb70b24771910ea849772ac4a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "TextModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_541daa35f17f445aa12b515e363b6052",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c8eb068de4f3468bb14e4bcafb81ccc8",
            "value": "haoyugao0517"
          }
        },
        "753c9e6740094d378235a36ffba85498": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c89885d0e6d54f4ab7c8212ad2b3e3b0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7df4812ff202410a9f862247d173fe18",
            "value": "‚Äá100/100‚Äá[02:15&lt;00:00,‚Äá‚Äá1.34it/s]"
          }
        },
        "75bc761f1c7e4121b222b9d290ef042a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7667454a4a824f56b9f65c74d1971005": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "791ff3bcfad5435a8c26d951239a8f82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6f4c3bd79dd45b2815dd44d783fb034",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ea39277bc7f64e788af63074ce27938f",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "7dbd4ffb3d6a4652a35606ab920bc243": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7df4812ff202410a9f862247d173fe18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7eac2c97629e4bedb14a06000d924a36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8eefd13762564bf39d0f15105bdf45c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c30773ff8fe465aba27c94548f35844": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dfe6527e59b4d7c9c9d19ea6d2adf3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e4288fa92fa4ca0a90f5027915409bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a02cab6068c6402aa716d3ed4b1d8626": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a76d11b7c17046288b2d32b7f945fbfe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbbf27c10bc9428d82cc9a432af6cfea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c13c7f0891574c0590666d423e340564": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbbf27c10bc9428d82cc9a432af6cfea",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_75bc761f1c7e4121b222b9d290ef042a",
            "value": "Training:‚Äá‚Äá29%"
          }
        },
        "c49fd462620b430c823d26ee5e35451f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c87d19692bcb4773b2c73143b7961ad5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_c49fd462620b430c823d26ee5e35451f",
            "style": "IPY_MODEL_5d84d606cbbd44a483ad2243f68c43d2",
            "tooltip": ""
          }
        },
        "c89885d0e6d54f4ab7c8212ad2b3e3b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8eb068de4f3468bb14e4bcafb81ccc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d03f6fd88bb04368b8e9075e1c6e19f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_676c8d61c501433f8e241c8a49d224d9",
              "IPY_MODEL_23fea0d29c744576a978ddb8bb70e192",
              "IPY_MODEL_753c9e6740094d378235a36ffba85498"
            ],
            "layout": "IPY_MODEL_7dbd4ffb3d6a4652a35606ab920bc243"
          }
        },
        "e91e3f489834436c83a2ff44f0f52560": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b2bbe3f4c8e4379bbaece8c0b4e7255",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3164117a82ce46d69d8bf3381ee57170",
            "value": "Connecting..."
          }
        },
        "ea39277bc7f64e788af63074ce27938f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecfe2bb7e1d240aebd3d31c438b62946": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5ca1c4bc34944938289957ab67521be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6f4c3bd79dd45b2815dd44d783fb034": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f95ea66851b7484884b16a111fe948ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31212847ea574d7aac892fb7aca7f1b3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ecfe2bb7e1d240aebd3d31c438b62946",
            "value": "Kaggle credentials successfully validated."
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
